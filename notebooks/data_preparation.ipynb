{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4042dfe0-368c-4f86-9040-83a8e326dd6d",
   "metadata": {},
   "source": [
    "# data_preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c8577f-2cdc-4dc4-b5fa-5ec057b1238f",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n",
    "\n",
    "In this section, we import the necessary libraries for file management, downloading datasets from Kaggle, and handling data structures. We also include __tqdm__ to monitor the progress of long-running operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0e89f1c-ed2b-4a32-97ca-8b47516a4b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baadd438-0524-430a-ab13-9a1b1bc94382",
   "metadata": {},
   "source": [
    "## 2. Downloading the Dataset\n",
    "\n",
    "This step involves fetching the FERPlus dataset using kagglehub. The code includes a retry mechanism that attempts the download up to 10 times in case of network interruptions. Once downloaded, the data is moved to a local data directory within the project structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a51be46-96dd-4830-a382-706c45c6078c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading to C:\\Users\\Asus\\.cache\\kagglehub\\datasets\\arnabkumarroy02\\ferplus\\3.archive...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▊                                                                         | 30.0M/487M [00:10<02:37, 3.04MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 31457280 bytes (479139832 bytes left)...\n",
      "Resuming download to C:\\Users\\Asus\\.cache\\kagglehub\\datasets\\arnabkumarroy02\\ferplus\\3.archive (31457280/510597112) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|██████▉                                                                       | 43.0M/487M [00:05<03:00, 2.58MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 45088768 bytes (465508344 bytes left)...\n",
      "Resuming download to C:\\Users\\Asus\\.cache\\kagglehub\\datasets\\arnabkumarroy02\\ferplus\\3.archive (45088768/510597112) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▋                                                                 | 79.0M/487M [00:13<02:31, 2.82MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 82837504 bytes (427759608 bytes left)...\n",
      "Resuming download to C:\\Users\\Asus\\.cache\\kagglehub\\datasets\\arnabkumarroy02\\ferplus\\3.archive (82837504/510597112) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▊                                                              | 99.0M/487M [00:09<03:12, 2.11MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming download from 103809024 bytes (406788088 bytes left)...\n",
      "Resuming download to C:\\Users\\Asus\\.cache\\kagglehub\\datasets\\arnabkumarroy02\\ferplus\\3.archive (103809024/510597112) bytes left.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 487M/487M [02:05<00:00, 3.24MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\HW\\AI_project\\main_project\\Facial-Expression-Recognition\\data\n"
     ]
    }
   ],
   "source": [
    "destination = Path.cwd().parent / \"data\"\n",
    "\n",
    "for i in range(10):\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(\"arnabkumarroy02/ferplus\")\n",
    "        break\n",
    "    except Exception:\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    raise Exception(\"Download failed after multiple attempts\")\n",
    "\n",
    "if destination.exists():\n",
    "    shutil.rmtree(destination)\n",
    "\n",
    "shutil.move(str(path), str(destination))\n",
    "\n",
    "print(destination)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46849a4-42f4-441b-859f-1eaec4bb8687",
   "metadata": {},
   "source": [
    "## 3. Organizing Data and Generating CSV\n",
    "\n",
    "Here, we scan the downloaded images and organize them. The script renames each image to include its emotion label, moves all files into a single directory, and generates a dataset.csv file. This CSV maps each filename to its corresponding emotion and numerical ID (e.g., Happy = 3, Sad = 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c00faf06-d2a9-446e-93cf-8fb76e225799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: D:\\HW\\AI_project\\main_project\\Facial-Expression-Recognition\\data\n",
      "Found 78293 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving Images: 100%|████████████████████████████████████████████████████████████| 78293/78293 [02:24<00:00, 542.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV saved at: D:\\HW\\AI_project\\main_project\\Facial-Expression-Recognition\\CSV files\\dataset.csv\n",
      "Cleaning up empty directories...\n",
      "Cleanup completed. Only images remain in 'data' folder.\n",
      "Process completed successfully.\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "project_root = os.path.dirname(current_dir)\n",
    "data_dir = os.path.join(project_root, 'data')\n",
    "csv_file_dir = os.path.join(project_root, 'CSV files')\n",
    "csv_file = os.path.join(csv_file_dir, 'dataset.csv')\n",
    "\n",
    "emotion_mapping = {\n",
    "    'angry': 0, 'anger': 0,\n",
    "    'disgust': 1, 'disgusted': 1,\n",
    "    'fear': 2, 'fearful': 2,\n",
    "    'happy': 3, 'happiness': 3,\n",
    "    'sad': 4, 'sadness': 4,\n",
    "    'surprise': 5, 'suprise': 5,\n",
    "    'neutral': 6, 'neutrality': 6,\n",
    "    'contempt': 7\n",
    "}\n",
    "\n",
    "data_rows = []\n",
    "files_to_process = []\n",
    "\n",
    "if os.path.exists(data_dir):\n",
    "    print(f\"Scanning directory: {data_dir}\")\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        if root == data_dir:\n",
    "            continue\n",
    "\n",
    "        for filename in files:\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                files_to_process.append((root, filename))\n",
    "\n",
    "    print(f\"Found {len(files_to_process)} images.\")\n",
    "\n",
    "    for root, filename in tqdm(files_to_process, desc=\"Moving Images\"):\n",
    "        label_name = os.path.basename(root)\n",
    "        label_id = emotion_mapping.get(label_name.lower(), -1)\n",
    "\n",
    "        new_filename = f\"{label_name}_{filename}\"\n",
    "\n",
    "        src_path = os.path.join(root, filename)\n",
    "        dst_path = os.path.join(data_dir, new_filename)\n",
    "\n",
    "        shutil.move(src_path, dst_path)\n",
    "\n",
    "        data_rows.append([new_filename, label_name, label_id])\n",
    "\n",
    "    with open(csv_file, mode='w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['filename', 'label', 'label_id'])\n",
    "        writer.writerows(data_rows)\n",
    "\n",
    "    print(f\"CSV saved at: {csv_file}\")\n",
    "\n",
    "    print(\"Cleaning up empty directories...\")\n",
    "\n",
    "    for root, dirs, files in os.walk(data_dir, topdown=False):\n",
    "        for name in dirs:\n",
    "            folder_path = os.path.join(root, name)\n",
    "            try:\n",
    "                os.rmdir(folder_path)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "    print(\"Cleanup completed. Only images remain in 'data' folder.\")\n",
    "    print(\"Process completed successfully.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Directory not found: {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6eb2eb-b603-40b0-9084-434c9d4f293c",
   "metadata": {},
   "source": [
    "## 4. Introducing \"Dirty\" Data for Testing\n",
    "\n",
    "To simulate real-world data issues and test our cleaning logic, we intentionally corrupt approximately 2% of the dataset. This includes:\n",
    "\n",
    "- __Missing Values:__ Removing labels or IDs.\n",
    "\n",
    "- __Typos:__ Introducing spelling errors in emotion names.\n",
    "\n",
    "- __Noise:__ Inserting invalid IDs or \"NaN\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7760f01e-e153-45de-9e54-ab5409b0ab75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done! 1581 rows (out of 78293) were modified in 'D:\\HW\\AI_project\\main_project\\Facial-Expression-Recognition\\CSV files\\dataset.csv'.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "def make_typo(text):\n",
    "    if len(text) > 2:\n",
    "        idx = random.randint(0, len(text) - 1)\n",
    "        return text[:idx] + text[idx + 1:]\n",
    "    return text\n",
    "\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        fieldnames = reader.fieldnames\n",
    "        rows = list(reader)\n",
    "\n",
    "    count = 0\n",
    "    total_rows = len(rows)\n",
    "\n",
    "    for row in rows:\n",
    "        if random.random() < 0.02:\n",
    "            count += 1\n",
    "            error = random.choice(['missing', 'typo', 'bad_id', 'noise'])\n",
    "\n",
    "            if error == 'missing':\n",
    "                if random.random() < 0.5:\n",
    "                    row['label'] = \"\"\n",
    "                else:\n",
    "                    row['label_id'] = \"\"\n",
    "            elif error == 'typo':\n",
    "                row['label'] = make_typo(row['label'])\n",
    "            elif error == 'bad_id':\n",
    "                row['label_id'] = random.choice(['99', '-1', '100'])\n",
    "            elif error == 'noise':\n",
    "                row['label_id'] = 'NaN'\n",
    "\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(rows)\n",
    "\n",
    "    print(f\"Done! {count} rows (out of {total_rows}) were modified in '{csv_file}'.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Error: File {csv_file_path} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe1926-6a2d-4548-92c3-41e6a79f245f",
   "metadata": {},
   "source": [
    "## 5. Data Cleaning and Validation\n",
    "\n",
    "The final step ensures the dataset is reliable for model training. The script validates each row in the CSV by checking for missing values, correcting or removing typos, and verifying that the label_id matches the emotion mapping. The result is saved as dataset_cleaned.csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "218ce14f-b785-4d8b-b7cc-73731b41afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows processed: 78293\n",
      "Rows removed: 1581\n",
      "Rows kept: 76712\n",
      "Cleaned data saved to: D:\\HW\\AI_project\\main_project\\Facial-Expression-Recognition\\CSV files\\dataset_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "output_csv = os.path.join(csv_file_dir, 'dataset_cleaned.csv')\n",
    "\n",
    "if os.path.exists(csv_file):\n",
    "    clean_rows = []\n",
    "    removed_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    with open(csv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        fieldnames = reader.fieldnames\n",
    "\n",
    "        for row in reader:\n",
    "            total_count += 1\n",
    "            label = row['label']\n",
    "            label_id = row['label_id']\n",
    "\n",
    "            is_valid = True\n",
    "\n",
    "            if not label or not label_id:\n",
    "                is_valid = False\n",
    "            elif label.lower() not in emotion_mapping:\n",
    "                is_valid = False\n",
    "            else:\n",
    "                try:\n",
    "                    lid = int(label_id)\n",
    "                    expected_id = emotion_mapping[label.lower()]\n",
    "\n",
    "                    if lid != expected_id:\n",
    "                        is_valid = False\n",
    "                except ValueError:\n",
    "                    is_valid = False\n",
    "\n",
    "            if is_valid:\n",
    "                clean_rows.append(row)\n",
    "            else:\n",
    "                removed_count += 1\n",
    "\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(clean_rows)\n",
    "\n",
    "    print(f\"Total rows processed: {total_count}\")\n",
    "    print(f\"Rows removed: {removed_count}\")\n",
    "    print(f\"Rows kept: {len(clean_rows)}\")\n",
    "    print(f\"Cleaned data saved to: {output_csv}\")\n",
    "\n",
    "else:\n",
    "    print(f\"File not found: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c5c1ae-fa57-4c38-9384-f7afafb587e6",
   "metadata": {},
   "source": [
    "## 6. Final Verification and Data Preview\n",
    "\n",
    "To conclude the data preparation phase, we load the processed dataset into a Pandas DataFrame to verify the results. This step allows us to inspect the first few rows of our cleaned CSV, ensuring that the formatting is correct and that the labels and IDs are properly aligned before moving on to the analysis and modeling stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3880d2d9-6a6d-43d3-9a3b-e8638407b4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               filename  label  label_id\n",
      "0  angry_fer0032225.png  angry         0\n",
      "1  angry_fer0032228.png  angry         0\n",
      "2  angry_fer0032239.png  angry         0\n",
      "3  angry_fer0032242.png  angry         0\n",
      "4  angry_fer0032258.png  angry         0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_csv)\n",
    "print(df.head(5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
